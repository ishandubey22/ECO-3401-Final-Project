The Excel Sheet - "deafault of credit cards.xls:  is the dataset that was used in the paper and the one we use in our R code as well.

The code operates in the following sequence:

Convert categorical variables (e.g., SEX, EDUCATION, PAY_0 to PAY_6) into factors.
Scale numeric predictors (e.g., credit limits, payment history).

Why we scale here:
KNN and neural networks rely on distance metrics (e.g., Euclidean distance), gradient-based optimization respectively.
If one feature has a larger numeric range than others (e.g., credit limit in thousands vs. age in tens), it will dominate the distance or learning process.
Scaling puts all numeric variables on a comparable scale, typically with mean = 0 and standard deviation = 1 (standardization), which helps the model learn more effectively.
For models like neural networks, scaling ensures faster and more stable convergence during training since the gradients become more balanced across layers.

Split data into training (70%) and test (30%) sets. For K-fold Cross validation a loop with K from 5 to 15. This code will take an hour to run (at least as per our machines).

Training the following models:
K-Nearest Neighbors (KNN)
Logistic Regression
Linear Discriminant Analysis (LDA)
Naive Bayes
Neural Network (via caret tuning)
Decision Tree (rpart)
Random Forest
Xgboost decision trees

ROC Curve: The ROC curve (Receiver Operating Characteristic curve) shows how well a model separates positive and negative classes by plotting the true positive rate against the
false positive rate at different thresholds. A model with a curve that hugs the top-left corner performs better, while a diagonal line means the model is no better than trying to 
randomly guess. The area under the curve (AUC) summarizes this performance—closer to 1 means strong predictive ability.

Area Ratio and SSM method:
Area Ratio for evaluating model performance and not error rate because of the imbalanced dataset. SSM or the smoothing sorting method to estimate the real probability of default 
and compare against our model. 

We generate test-set predictions from all models.
Computes the following evaluation metrics for each:
AUC (Area Under the ROC Curve)
Calibration R² (based on smoothed observed probabilities, namely, SSM)

Calibration Intercept & Slope
Area Ratio (lift-based metric comparing model vs. perfect vs. random baseline) - 

Smoothing Function - From the paper to locally smooth true default outcomes for evaluating probability calibration.
